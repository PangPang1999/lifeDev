# Start

## 笔记概述

1. 基础认知

    - 语言模型（LLM）
    - Token 与长度/费用
    - 上下文窗口（Context Window）与截断

2. 模型与提示

    - 模型选择：能力 / 成本 / 延迟
    - 模型参数：温度、Top-p 等
    - Prompt Engineering：角色、约束、示例

3. 项目实战

    - 主题公园问答 Chatbot：快速检索与导航
    - 客户反馈分析：提炼可执行洞察、辅助秒级决策
    - 开源集成：可在任意环境运行的方案

4. 工程与工具

    - Clean Architecture 与最佳实践
    - 现代栈：Bun、Tailwind、shadcn/ui、Prisma、Ollama（字幕原拼：Bunn / Shatian / Olamo）
    - 目标：稳定、可维护、可部署

## 学前要求

1. 现代 JavaScript / TypeScript

    - 箭头函数（arrow functions）、解构赋值（destructuring）
    - Promise、async/await 异步流程
    - 模块化与基础类型（TS）

2. React 入门能力

    - 组件与 JSX
    - 基本状态与副作用：useState、useEffect

3. 可选加分项（非必需）

    - 简单后端与数据库常识

4. 学习方式与预期

    - 逐步讲解、从零起步
    - 目标：把 AI 融入应用，让产品更聪明更好用

## 笔记结构

> 简述：先学清楚原理，再做真实项目；从前后端分离的全栈架构，到 prompt 工程与数据库驱动的复杂功能，再到开源模型的集成，逐层递进，强调动手与理解。

**内容**

1. 学习定位

    - 非“速成/娱乐型”课程
    - 注重理解原理与架构
    - 目标：能独立构建有用的 AI 功能

2. 课程结构

    - Section 1：基础

        - 语言模型原理（LLM 能力与边界）
        - Token、上下文窗口
        - 模型参数（温度等）与调用方式

    - Section 2：全栈环境搭建

        - 前后端分离而非 Next.js 一体化
        - 技术栈：Express + React + Tailwind + shadcn/ui
        - 学习前后端交互逻辑

    - Section 3：Chatbot 项目

        - 问答机器人
        - 从后端开始 → 重构为 Clean Architecture → 再到前端交互
        - 循环迭代功能与 UI

    - Section 4：Prompt Engineering

        - 提供上下文
        - 控制输出格式
        - 使用示例减少幻觉
        - 错误处理与稳定性提升

    - Section 5：产品评论总结器

        - 构建完整系统：数据库 + Prisma migrations
        - 复杂度升级 → 引入更多工程化技巧
        - 技术可迁移至其他 AI 功能（不限于文本总结）

    - Section 6：开源模型集成

        - 为什么重要
        - 如何查找与运行本地模型
        - 集成至应用 → 打破对商业 API 的依赖

## 开发环境准备

1. `Node.js` 版本 `22.19`
2. 编辑器选择 VS Code
3. 其余工具按课程进度再装

# Introduction to AI Models

## AI Engineer 是什么

> 简述：AI Engineer 用现成大模型做功能落地（像用数据库一样调用与集成），而不是训练模型本身。目标是把 AI 变成可靠、可维护的产品能力。

**知识树**

1. 角色定义与边界

    - AI Engineer：集成预训练 LLM，设计数据流与接口，保障质量/成本/延迟。
    - ML Engineer：数据清洗、模型训练与调参、训练管线与研究。

2. 行业趋势与需求

    - 模型/工具/API 快速迭代 → 新岗位与新期望。
    - 企业需要“把 AI 变成实用功能”的工程师。
    - 价值取向：提效、降本、提升体验与转化率。

3. 常见功能范式

    - 摘要：电商评论速览（例：亚马逊）→ 快速决策。
    - 生成：营销邮件/文案（例：ActiveCampaign）→ 减少冷启动。
    - 翻译/本地化：社媒一键翻译 → 自动识别语言与场景。
    - 审核：平台自动识别垃圾/不当内容（例：YouTube/Twitch）。
    - 工单自动化：分类、优先级与路由（例：Freshdesk）。
    - 场景化问答：特定实体助手（例：Redfin 房源问答）。
    - 共同要点：正确性、稳定性、时延、成本、可观测性。

## 什么是大模型

> 简述：大语言模型（LLM）通过海量数据的训练来理解和生成自然语言，预测性强但没有真正的理解力。它们通过数学和概率生成输出，依赖训练数据的质量，影响结果的可靠性与偏见。课程将探讨如何与这些模型互动，以及它们的局限性。

**知识树**

1. 大语言模型的定义

    - 语言模型是一个训练出来理解并生成自然语言的系统。
    - 通过大量文本（书籍、文章、论坛、代码等）学习语言中的统计模式（语法、语调、常识等）。
    - 模型的核心是预测下一个词汇或句子，而不是理解其含义。

2. 模型规模与参数

    - 大型语言模型通常包含数十亿参数，这些参数代表语言模式（如语法、风格等）。
    - 参数的数量和计算能力决定了模型的“规模”和能力。
    - 输出流畅、结构良好，但本质上只是基于数据中的概率计算，不具备理解或意识。

3. 模型的局限性

    - 无理解能力：模型并不“理解”其生成的内容，它基于统计数据生成输出。
    - 输出差异：同样的问题可能会得到不同的回答，因为每次生成的结果都是基于概率的。
    - 训练数据的质量至关重要：偏差、不准确或低质量的数据会影响模型的输出，可能产生误导性的或错误的回答。

4. 训练与应用的挑战

    - 数据质量：模型依赖于清晰、高质量的数据。如果数据不可靠，模型的输出也会出现问题。
    - 代码生成的风险：模型训练于大量公共代码库，代码虽然表面上看起来干净、规范，但可能包含错误、过时的实践或反模式。
    - 训练的成本与资源：训练一个大型模型需要巨大的计算资源，仅有少数公司能够承担这项成本。作为开发者，我们不需要训练模型，只需学会如何与这些模型互动，理解其局限性，并将它们集成到实际应用中。

5. 模型与应用的关系

    - 与数据库类比：开发者不需要构建数据库引擎，只需了解如何使用它。同样，开发者也不需要训练自己的大模型，而是要学会如何通过提示与模型互动，如何利用现有模型来构建智能功能。
    - 应用实践：开发者的任务是利用这些已训练好的大模型，构建更加智能、有效的应用功能。

## 大模型能用来做什么

> 简述：大语言模型（LLM）作为辅助工具，在现代应用中主要负责文本处理任务，如内容生成、分类、翻译、提取信息和构建聊天机器人等。它们通过输入输出交互方式提升用户体验，而通常不作为核心模块。

**知识树**

1. 总结功能

    - 任务：将长文本（如产品评论、文章）浓缩为简短总结，帮助用户快速获取关键信息。
    - 例子：电商平台（如亚马逊）使用 LLM 来快速展示产品评价摘要，提升购买决策效率。

2. 内容生成

    - 任务：根据简短提示生成完整文本内容。
    - 例子：
        - 生成电子邮件：根据关键词或上下文自动生成邮件内容。
        - 产品描述生成：自动为商品生成描述，提升内容创建效率。
        - 社交媒体内容：自动撰写推文、广告文案，提升社交平台的互动与推广效果。

3. 文本分类

    - 任务：基于输入的文本，将其分配到预定类别。
    - 例子：
        - 垃圾邮件识别：判断邮件是否为垃圾邮件。
        - 情感分析：分析评论或文章的情感倾向（如正面或负面）。
        - 支持工单分类：自动将客户支持请求分类并分配给相应的部门（如账单、登录问题等）。

4. 数据结构化提取

    - 任务：从非结构化文本（如 PDF 文件）中提取关键信息，并转换为结构化数据。
    - 例子：提取发票号码、金额、地址等信息，用于自动化文档处理和数据录入。

5. 机器翻译

    - 任务：将文本从一种语言翻译为另一种语言。
    - 例子：Twitter、iOS 提供实时翻译功能，用户看到外语内容时，模型自动翻译成目标语言。

6. 聊天机器人

    - 任务：构建能够回答用户问题的聊天机器人。
    - 例子：根据用户数据或业务文档，提供实时客户支持和信息查询，提升互动体验。

7. 输入输出模式

    - 大语言模型的工作方式：文本输入 → **文本输出**。
    - 输出形式不限于文本，还可以是 JSON 对象、数组、数字，甚至图像等，取决于任务需求。例如，输出结构化数据供后端处理和存储。

## 理解 Tokens 和上下文窗口

> 简述：Tokens 是大语言模型处理文本的基本单位，控制和优化 tokens 使用对于节省成本、避免超出限制非常关键，尤其在处理长文本或多轮对话时，合理管理 tokens 和上下文窗口能有效提升性能和体验。

**知识树**

1. 什么是 Token

    - 定义：Tokens 是语言模型处理文本的基本单位，介于字符和单词之间。它们可以是完整的单词、部分单词、标点符号，甚至空格或 emoji。
    - 例如：“ChatGPT is amazing!” 被分解为多个 tokens，如 "Chat", "GPT", "is", "amazing", "!"。
    - [OpenAI Tokenizer](https://platform.openai.com/tokenizer) 可以帮助分析输入文本的 token 数量。

2. Token 的作用与成本

    - 成本影响：每个 token 的处理都涉及费用。长文本处理时，token 数量直接决定了成本。
    - 模型选择：选择模型时不仅要考虑性能，还要评估与任务需求匹配的 token 成本，避免使用不必要的大模型。
    - 成本管理：生成大量内容（如总结长文档）时，token 使用和成本可能会快速增加。

3. 上下文窗口（Context Window）

    - 定义：上下文窗口是指模型能够处理的最大 token 数量，包括输入提示、模型输出和对话历史。
    - 限制：当输入超出上下文窗口时，模型可能会在未完成句子的情况下停止输出。
    - 模型差异：不同模型的上下文窗口大小不同，选择合适的窗口大小能够确保流畅的用户体验。例如，某些任务可能不需要最大的上下文窗口。

4. 选择模型与需求匹配

    - 合理选择：并非所有应用都需要最强大的模型。根据实际需求选择合适的模型和上下文窗口大小，能够平衡成本和性能。
    - 实例：Mistral 模型对于博客总结或支持工单分类等任务非常适合，而不需要选择更高端的模型。

5. Token 计数与成本管理

    - 下节课将介绍如何编程计数 tokens，以便估算成本并确保请求在上下文窗口限制内。

## 计算 Tokens 的方法

> 简述：通过编程计算 tokens，可以估算与管理模型的使用成本，确保请求不会超出限制。使用 Tiktoken 库，可以轻松计算输入文本的 token 数量。本节介绍一个简单的计算 token 工具 tiktoken

**知识树**

1. 初始化项目与安装依赖

    - 创建一个项目目录并初始化 `package.json` 文件。
    - 安装 `tiktoken` 库：用于分解文本为 tokens。
        ```sh
        mkdir playground # 创建项目目录
        cd playground
        npm init --y # 初始化项目，创建 `package.json` 文件。
        npm i tiktoken # 安装 `tiktoken` 库：用于分解文本为 tokens。
        ```

2. Tiktoken 使用

    - 在 VS Code 中创建 `index.js` 文件
    - 导入并使用 `getEncoding` 函数获取编码器。
    - 编码器将文本转换为 tokens。
    - 使用 `node index.js` 运行代码

3. 处理语法

    - 该初始化方式创建项目后，默认为 CommonJS 模块格式，建议调整为 module 模式，确保代码按预期运行。

**代码示例**

1. 创建 `index.js` 文件并导入 `tiktoken`

    ```javascript
    // index.js
    import { getEncoding } from "tiktoken";

    const encoding = getEncoding("cl100k_base"); // 使用默认编码
    const tokens = encoding.encode(
    	"hello world this is the first test of tiktoken library"
    );
    console.log(tokens);

    // 输出示例
    ╰─ node index.js
    Uint32Array(11) [
      15339, 1917,   420,
        374,  279,  1176,
       1296,  315, 87272,
       5963, 6875
    ]
    ```

    - 通过 `getEncoding` 获取编码器，将输入文本分解为多个 tokens，并输出。

2. 处理模块语法错误：

    如果使用 ES 模块导入时遇到错误，可以调整 `package.json` 配置：

    ```json
    // package.json
    {
    	"name": "playground",
    	"version": "1.0.0",
    	"description": "",
    	"type": "module", // 新增（修改后删除注释）
    	"main": "index.js",
    	"scripts": {
    		"test": "echo \"Error: no test specified\" && exit 1"
    	},
    	"keywords": [],
    	"author": "",
    	"license": "ISC",
    	"dependencies": {
    		"tiktoken": "^1.0.22"
    	}
    }
    ```

    - 设置 `type` 为 `module`，以便使用 ES 模块语法。

## 选择合适的 AI 模型

> 简述：选择合适的 AI 模型要基于多种因素，如任务复杂度、响应速度、输入输出类型、成本、上下文窗口和隐私需求。本节将介绍选择模型时需要考虑的关键标准，帮助开发者做出最合适的决策。

**知识树**

0. openAI 模型比较地址

    - https://platform.openai.com/docs/models/compare

1. 任务复杂度与模型智能

    - 需要解决复杂问题时，选择具有较强推理能力的模型。
    - 对于简单任务，如文本提取、分类或总结，较小的模型也能胜任，且通常更高效。

2. 响应速度

    - 较大的模型通常生成内容较慢，尤其是生成较长的输出时。
    - 如果应用需要实时响应（如自动补全、快速总结或短答），则应选择速度较快的模型。

3. 输入输出类型

    - 文本：大多数模型支持文本输入输出。
    - 多模态模型（LMMs）：处理图像、音频、视频或其组合的模型。如果应用需要处理图像内容（如图像描述），则需选择支持多模态的模型。

4. 成本管理

    - 根据应用需求，选择合适的模型避免超出预算。
    - 处理长文档或生成大量内容时，token 数量较多，成本会快速上升。应评估模型的输入和输出费用，选择性价比高的模型。

5. 上下文窗口（Context Window）

    - 上下文窗口决定了模型一次可以处理的文本量，包括输入、输出和对话历史。
    - 对于长文档总结、大规模代码分析或多轮对话，需要选择具有大上下文窗口的模型。

6. 隐私需求

    - 如果应用处理敏感数据（如医疗记录），使用开源自托管的模型可能更安全，避免将数据发送到外部服务器。
    - 将在课程后面深入讨论自托管模型的使用。

7. 模型选择时的具体考量

    - 多模态 vs 单模态：某些模型支持多模态输入（如文本和图像），但可能仅支持文本输出；而其他模型可能专注于生成图像或其他类型的输出。
    - 模型定价：选择合适的模型时，应比较不同模型的定价结构（输入与输出的费用）来控制成本。
    - 知识截止日期：模型的知识库更新时间影响其对最新信息的掌握，可能有些模型因训练数据的截止日期较早而无法获取最新信息，但可能更适合某些特定任务。

8. 模型比较工具

    - 通过对比不同模型的能力、成本和特点，选择最适合的模型。
    - 可在平台提供的模型比较工具中查看具体模型的上下文窗口、输出 token 数量限制及价格等关键参数。

## 模型设置与输出控制

> 简述：用少量开关，决定模型“产什么”“多随机”“多长”“怎么追踪”。学会这些设置，输出才可解析、可控、可复现。

**知识树**

0. openai 调试说明

    - 地址：https://platform.openai.com/chat/edit?models=gpt-4.1
    - 缺点：付费麻烦，国内充值 openai 存在巨大阻力
    - 优点：相较于 deepseek，openai 更方便在线调试参数进行测试，本节基于 openai 进行调试

1. 输出格式（text / JSON object / JSON schema）

    - text：直接自然语言，读起来顺滑，但不便机器处理。
    - JSON object：要求“结构化回复”，后端易解析。示例提示：`请以JSON返回：{ "benefits": string[] }`。
        - 如：`give me 3 benefit of exercising as a json object`
    - JSON schema：先定义字段与类型（如 exercise.benefits 为 string\[]、required 等），再让模型按此生成，更稳更可控。适合“必须合规”的接口回复。
        - 如：点击 Generate，配合`{"exercise":{"benifits":[]}`，可快速生成标准的格式

2. 随机性与风格（temperature 与 top_p）

    - temperature：数值越高越发散，越低越确定。
        - 总结/事实问答用低值（≈0.2–0.4）
        - 脑暴/文案用高值（≈0.7–1.0）
        - 极端值易胡言乱语，避免接近 0 或 2
    - top_p：按“累积概率”裁剪可选词。低 top_p 只取最可能的词，更保守
    - 通常只调一个旋钮（temperature 或 top_p），不同时大改；拿不准就只调 temperature。

3. 长度与上下文（max output tokens 与 context window）

    - max output tokens：限制“单次回复的最大长度”。值太小会中途截断。
        - 两种缓解：
            - 提高上限；
            - 在提示中明确要求“写完整答案，不要中途截断”。
                - 如：`write a story about a robot in 50 tokens or less. write a complete answer without cutting off mind-sentence`
    - context window：一次请求能承载的总 token 上限＝输入 + 输出 +（对话历史）。做长文档总结/长会话，需选更大的窗口。注意：大窗口 ≠ 无限输出，仍受 max output tokens 约束。

4. 可观测性与成本（日志、价格、速度、模态）

    - 日志（store logs）：默认开启。Dashboard→Logs 可看每次请求的输入/输出、模型、时间、token 用量、温度、响应 ID 等；响应 ID 可用于会话追踪与调试。处理敏感数据时，评估是否需要关闭或做脱敏。
    - 价格：输入与输出分开计费；长文档或长回复成本增长快。先估 token，再定格式与上限。
    - 速度：更强模型常更慢；实时体验（自动补全、短答）优先选快模型并控制输出长度。
    - 模态：有的仅文本输出；有的支持图像输入；需要“生成图像”需选专门图像模型。选型以“需求 → 能力 → 成本/延迟”倒推，而非盲目追新。

