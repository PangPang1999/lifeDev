# Start

## 笔记概述

1. 基础认知

    - 语言模型（LLM）
    - Token 与长度/费用
    - 上下文窗口（Context Window）与截断

2. 模型与提示

    - 模型选择：能力 / 成本 / 延迟
    - 模型参数：温度、Top-p 等
    - Prompt Engineering：角色、约束、示例

3. 项目实战

    - 主题公园问答 Chatbot：快速检索与导航
    - 客户反馈分析：提炼可执行洞察、辅助秒级决策
    - 开源集成：可在任意环境运行的方案

4. 工程与工具

    - Clean Architecture 与最佳实践
    - 现代栈：Bun、Tailwind、shadcn/ui、Prisma、Ollama（字幕原拼：Bunn / Shatian / Olamo）
    - 目标：稳定、可维护、可部署

## 学前要求

1. 现代 JavaScript / TypeScript

    - 箭头函数（arrow functions）、解构赋值（destructuring）
    - Promise、async/await 异步流程
    - 模块化与基础类型（TS）

2. React 入门能力

    - 组件与 JSX
    - 基本状态与副作用：useState、useEffect

3. 可选加分项（非必需）

    - 简单后端与数据库常识

4. 学习方式与预期

    - 逐步讲解、从零起步
    - 目标：把 AI 融入应用，让产品更聪明更好用

## 笔记结构

> 简述：先学清楚原理，再做真实项目；从前后端分离的全栈架构，到 prompt 工程与数据库驱动的复杂功能，再到开源模型的集成，逐层递进，强调动手与理解。

**内容**

1. 学习定位

    - 非“速成/娱乐型”课程
    - 注重理解原理与架构
    - 目标：能独立构建有用的 AI 功能

2. 课程结构

    - Section 1：基础

        - 语言模型原理（LLM 能力与边界）
        - Token、上下文窗口
        - 模型参数（温度等）与调用方式

    - Section 2：全栈环境搭建

        - 前后端分离而非 Next.js 一体化
        - 技术栈：Express + React + Tailwind + shadcn/ui
        - 学习前后端交互逻辑

    - Section 3：Chatbot 项目

        - 问答机器人
        - 从后端开始 → 重构为 Clean Architecture → 再到前端交互
        - 循环迭代功能与 UI

    - Section 4：Prompt Engineering

        - 提供上下文
        - 控制输出格式
        - 使用示例减少幻觉
        - 错误处理与稳定性提升

    - Section 5：产品评论总结器

        - 构建完整系统：数据库 + Prisma migrations
        - 复杂度升级 → 引入更多工程化技巧
        - 技术可迁移至其他 AI 功能（不限于文本总结）

    - Section 6：开源模型集成

        - 为什么重要
        - 如何查找与运行本地模型
        - 集成至应用 → 打破对商业 API 的依赖

## 开发环境准备

1. `Node.js` 版本 `22.19`
2. 编辑器选择 VS Code
3. 其余工具按课程进度再装

# Introduction to AI Models

## AI Engineer 是什么

> 简述：AI Engineer 用现成大模型做功能落地（像用数据库一样调用与集成），而不是训练模型本身。目标是把 AI 变成可靠、可维护的产品能力。

**知识树**

1. 角色定义与边界

    - AI Engineer：集成预训练 LLM，设计数据流与接口，保障质量/成本/延迟。
    - ML Engineer：数据清洗、模型训练与调参、训练管线与研究。

2. 行业趋势与需求

    - 模型/工具/API 快速迭代 → 新岗位与新期望。
    - 企业需要“把 AI 变成实用功能”的工程师。
    - 价值取向：提效、降本、提升体验与转化率。

3. 常见功能范式

    - 摘要：电商评论速览（例：亚马逊）→ 快速决策。
    - 生成：营销邮件/文案（例：ActiveCampaign）→ 减少冷启动。
    - 翻译/本地化：社媒一键翻译 → 自动识别语言与场景。
    - 审核：平台自动识别垃圾/不当内容（例：YouTube/Twitch）。
    - 工单自动化：分类、优先级与路由（例：Freshdesk）。
    - 场景化问答：特定实体助手（例：Redfin 房源问答）。
    - 共同要点：正确性、稳定性、时延、成本、可观测性。

## 什么是大模型

> 简述：大语言模型（LLM）通过海量数据的训练来理解和生成自然语言，预测性强但没有真正的理解力。它们通过数学和概率生成输出，依赖训练数据的质量，影响结果的可靠性与偏见。课程将探讨如何与这些模型互动，以及它们的局限性。

**知识树**

1. 大语言模型的定义

    - 语言模型是一个训练出来理解并生成自然语言的系统。
    - 通过大量文本（书籍、文章、论坛、代码等）学习语言中的统计模式（语法、语调、常识等）。
    - 模型的核心是预测下一个词汇或句子，而不是理解其含义。

2. 模型规模与参数

    - 大型语言模型通常包含数十亿参数，这些参数代表语言模式（如语法、风格等）。
    - 参数的数量和计算能力决定了模型的“规模”和能力。
    - 输出流畅、结构良好，但本质上只是基于数据中的概率计算，不具备理解或意识。

3. 模型的局限性

    - 无理解能力：模型并不“理解”其生成的内容，它基于统计数据生成输出。
    - 输出差异：同样的问题可能会得到不同的回答，因为每次生成的结果都是基于概率的。
    - 训练数据的质量至关重要：偏差、不准确或低质量的数据会影响模型的输出，可能产生误导性的或错误的回答。

4. 训练与应用的挑战

    - 数据质量：模型依赖于清晰、高质量的数据。如果数据不可靠，模型的输出也会出现问题。
    - 代码生成的风险：模型训练于大量公共代码库，代码虽然表面上看起来干净、规范，但可能包含错误、过时的实践或反模式。
    - 训练的成本与资源：训练一个大型模型需要巨大的计算资源，仅有少数公司能够承担这项成本。作为开发者，我们不需要训练模型，只需学会如何与这些模型互动，理解其局限性，并将它们集成到实际应用中。

5. 模型与应用的关系

    - 与数据库类比：开发者不需要构建数据库引擎，只需了解如何使用它。同样，开发者也不需要训练自己的大模型，而是要学会如何通过提示与模型互动，如何利用现有模型来构建智能功能。
    - 应用实践：开发者的任务是利用这些已训练好的大模型，构建更加智能、有效的应用功能。

## 大模型能用来做什么

> 简述：大语言模型（LLM）作为辅助工具，在现代应用中主要负责文本处理任务，如内容生成、分类、翻译、提取信息和构建聊天机器人等。它们通过输入输出交互方式提升用户体验，而通常不作为核心模块。

**知识树**

1. 总结功能

    - 任务：将长文本（如产品评论、文章）浓缩为简短总结，帮助用户快速获取关键信息。
    - 例子：电商平台（如亚马逊）使用 LLM 来快速展示产品评价摘要，提升购买决策效率。

2. 内容生成

    - 任务：根据简短提示生成完整文本内容。
    - 例子：
        - 生成电子邮件：根据关键词或上下文自动生成邮件内容。
        - 产品描述生成：自动为商品生成描述，提升内容创建效率。
        - 社交媒体内容：自动撰写推文、广告文案，提升社交平台的互动与推广效果。

3. 文本分类

    - 任务：基于输入的文本，将其分配到预定类别。
    - 例子：
        - 垃圾邮件识别：判断邮件是否为垃圾邮件。
        - 情感分析：分析评论或文章的情感倾向（如正面或负面）。
        - 支持工单分类：自动将客户支持请求分类并分配给相应的部门（如账单、登录问题等）。

4. 数据结构化提取

    - 任务：从非结构化文本（如 PDF 文件）中提取关键信息，并转换为结构化数据。
    - 例子：提取发票号码、金额、地址等信息，用于自动化文档处理和数据录入。

5. 机器翻译

    - 任务：将文本从一种语言翻译为另一种语言。
    - 例子：Twitter、iOS 提供实时翻译功能，用户看到外语内容时，模型自动翻译成目标语言。

6. 聊天机器人

    - 任务：构建能够回答用户问题的聊天机器人。
    - 例子：根据用户数据或业务文档，提供实时客户支持和信息查询，提升互动体验。

7. 输入输出模式

    - 大语言模型的工作方式：文本输入 → **文本输出**。
    - 输出形式不限于文本，还可以是 JSON 对象、数组、数字，甚至图像等，取决于任务需求。例如，输出结构化数据供后端处理和存储。

## 理解 Tokens 和上下文窗口

> 简述：Tokens 是大语言模型处理文本的基本单位，控制和优化 tokens 使用对于节省成本、避免超出限制非常关键，尤其在处理长文本或多轮对话时，合理管理 tokens 和上下文窗口能有效提升性能和体验。

**知识树**

1. 什么是 Token

    - 定义：Tokens 是语言模型处理文本的基本单位，介于字符和单词之间。它们可以是完整的单词、部分单词、标点符号，甚至空格或 emoji。
    - 例如：“ChatGPT is amazing!” 被分解为多个 tokens，如 "Chat", "GPT", "is", "amazing", "!"。
    - [OpenAI Tokenizer](https://platform.openai.com/tokenizer) 可以帮助分析输入文本的 token 数量。

2. Token 的作用与成本

    - 成本影响：每个 token 的处理都涉及费用。长文本处理时，token 数量直接决定了成本。
    - 模型选择：选择模型时不仅要考虑性能，还要评估与任务需求匹配的 token 成本，避免使用不必要的大模型。
    - 成本管理：生成大量内容（如总结长文档）时，token 使用和成本可能会快速增加。

3. 上下文窗口（Context Window）

    - 定义：上下文窗口是指模型能够处理的最大 token 数量，包括输入提示、模型输出和对话历史。
    - 限制：当输入超出上下文窗口时，模型可能会在未完成句子的情况下停止输出。
    - 模型差异：不同模型的上下文窗口大小不同，选择合适的窗口大小能够确保流畅的用户体验。例如，某些任务可能不需要最大的上下文窗口。

4. 选择模型与需求匹配

    - 合理选择：并非所有应用都需要最强大的模型。根据实际需求选择合适的模型和上下文窗口大小，能够平衡成本和性能。
    - 实例：Mistral 模型对于博客总结或支持工单分类等任务非常适合，而不需要选择更高端的模型。

5. Token 计数与成本管理

    - 下节课将介绍如何编程计数 tokens，以便估算成本并确保请求在上下文窗口限制内。

